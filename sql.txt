nonohhhiSQL QUERY ..


CREATE TABLE devices(
  id INT IDENTITY(1,1) PRIMARY KEY,
  device_id NVARCHAR(255),
  device_number1 NVARCHAR(255),
  device_number2 NVARCHAR (255),
  device_number3 NVARCHAR (255),
  device_number4 NVARCHAR (255),
  parameter_name1 NVARCHAR(255),
  parameter_name2 NVARCHAR(255),
  parameter_name3 NVARCHAR(255),
  parameter_name4 NVARCHAR(255),
  parameter_name5 NVARCHAR(255),
  parameter_name6 NVARCHAR(255),
  parameter_name7 NVARCHAR(255),
  parameter_name8 NVARCHAR(255),
  parameter_name9 NVARCHAR(255),
  parameter_name10 NVARCHAR(255),
  device_name NVARCHAR(255),
  parameter1 NVARCHAR(255),
  parameter2 NVARCHAR(255),
  parameter3 NVARCHAR(255),
  parameter4 NVARCHAR(255),
  parameter5 NVARCHAR(255),
  parameter6 NVARCHAR(255),
  parameter7 NVARCHAR(255),
  parameter8 NVARCHAR(255),
  parameter9 NVARCHAR(255),
  parameter10 NVARCHAR(255),
  user_id INT,
  created_at DATETIME DEFAULT GETDATE(),
  updated_at DATETIME DEFAULT GETDATE()
);

CREATE TABLE Config
(
    id INT PRIMARY KEY IDENTITY(1,1),
    parameter_name NVARCHAR(255),
    upper_limit DECIMAL(10,4),
    lower_limit DECIMAL(10,4),
    createdAt DATETIME,
    updatedAt DATETIME
);


INSERT INTO Config (parameter_name, upper_limit, lower_limit)
VALUES ('parameter1', 10, 8),
       ('parameter2', 5, 4),
       ('parameter3', 20.2, 0),
       ('parameter4', 12.8, 0),
       ('parameter5', 18.9, 0),
       ('parameter6', 22.1, 0),
       ('parameter7', 16.7, 0),
       ('parameter8', 9.3, 0),
       ('parameter9', 11.6, 0),
       ('parameter10', 14.2, 0);



IDS: https://drive.google.com/drive/folders/1LnjQEqID65m3qKEK4PwyXecGE4QeeN5M?usp=sharing

IDS AVG : https://drive.google.com/drive/folders/1H4gzaV54opIDYxW9fjD-jGQD8_5lrKmw?usp=sharing

https://drive.google.com/drive/folders/1H4gzaV54opIDYxW9fjD-jGQD8_5lrKmw?usp=sharing


DO $$ 
DECLARE
    r RECORD;
    search_term TEXT := 'John';  -- Replace with your search term
    query TEXT;
BEGIN
    -- Loop over all tables and JSON columns
    FOR r IN 
        SELECT table_name, column_name
        FROM information_schema.columns
        WHERE data_type IN ('json', 'jsonb')
    LOOP
        -- Dynamically generate query for full-text search across JSON data
        query := format('
            SELECT ''%s'' AS table_name, * 
            FROM %I 
            WHERE to_tsvector(''english'', 
                string_agg(kv.key || '':'|| kv.value, '' ')) 
                @@ plainto_tsquery(''english'', %L)
            FROM jsonb_each_text(%I.%I) AS kv', 
            r.table_name, r.table_name, search_term, r.column_name);
        
        -- Execute the dynamically generated query
        EXECUTE query;
    END LOOP;
END $$;


const express = require('express');
const { Client } = require('pg');
const app = express();
const port = 3000;

// Setup PostgreSQL client
const client = new Client({
    host: 'localhost', // Update with your PostgreSQL host
    port: 5432,        // Update with your PostgreSQL port
    user: 'your_user', // Update with your PostgreSQL user
    password: 'your_password', // Update with your PostgreSQL password
    database: 'your_database'  // Update with your database name
});

// Connect to PostgreSQL database
client.connect();

// Route for searching a specific value across all JSON columns in the database
app.get('/search', async (req, res) => {
    const searchValue = req.query.value;  // Take search value from query params

    if (!searchValue) {
        return res.status(400).send('Please provide a search value.');
    }

    try {
        // Get all JSON columns from the database
        const result = await client.query(`
            SELECT table_name, column_name
            FROM information_schema.columns
            WHERE data_type IN ('json', 'jsonb')
        `);

        // Loop through all tables and columns and dynamically search each JSON column
        const queries = result.rows.map(row => {
            const { table_name, column_name } = row;

            return client.query(`
                SELECT '${table_name}' AS table_name, * 
                FROM ${table_name}
                WHERE EXISTS (
                    SELECT 1
                    FROM jsonb_each_text(${table_name}.${column_name}) AS kv
                    WHERE kv.value = $1
                )
            `, [searchValue]);
        });

        // Execute all queries concurrently
        const searchResults = await Promise.all(queries);

        // Flatten the results and send them as the response
        const flattenedResults = searchResults.flatMap(result => result.rows);
        res.json(flattenedResults);
    } catch (error) {
        console.error('Error querying database:', error);
        res.status(500).send('An error occurred while searching the database.');
    }
});

// Start the Express server
app.listen(port, () => {
    console.log(`Server is running at http://localhost:${port}`);
});





latest
const express = require('express');
const { Client } = require('pg');
const app = express();
const port = 3000;

// Setup PostgreSQL client
const client = new Client({
    host: 'localhost', // PostgreSQL host
    port: 5432,        // PostgreSQL port
    user: 'your_user', // PostgreSQL user
    password: 'your_password', // PostgreSQL password
    database: 'your_database'  // Database name
});

// Connect to PostgreSQL database
client.connect();

// Route for searching a specific value in JSON column of a single table using regex
app.get('/search', async (req, res) => {
    const { search, table, column } = req.query;  // Retrieve search text, table, and column from query params

    if (!search || !table || !column) {
        return res.status(400).send('Please provide search term, table, and column.');
    }

    try {
        // Use regex to search within the JSONB column (cast to text)
        const query = `
            SELECT * 
            FROM ${table}
            WHERE ${column}::text ~* $1  -- Using case-insensitive regex match for JSONB field
        `;

        // Perform the query with the provided search value
        const result = await client.query(query, [`${search}`]);

        // Return the search results
        res.json(result.rows);
    } catch (error) {
        console.error('Error querying database:', error);
        res.status(500).send('An error occurred while searching the database.');
    }
});

// Start the Express server
app.listen(port, () => {
    console.log(`Server is running at http://localhost:${port}`);
});


CREATE INDEX bpsi.data_pg_trm_idx ON bpsi.intv_XXXXXXXX USING GIN (data gin_trgm_ops); 

latest today

CREATE INDEX data_pg_trgm_idx ON intv_XXXXXXXX USING GIN (data gin_trgm_ops);


CREATE INDEX data_jsonb_idx ON intv_XXXXXXXX USING GIN (data jsonb_path_ops);

const result = await pool.query(
  `SELECT * FROM intv_XXXXXXXX WHERE data::text ILIKE $1 LIMIT 50 OFFSET 0`,
  [`%${query}%`]
);


const { Pool } = require('pg');
const pool = new Pool();

app.get('/stream-search', async (req, res) => {
  const client = await pool.connect();
  try {
    const query = `SELECT * FROM intv_XXXXXXXX WHERE data::text ILIKE $1`;
    const stream = client.query(new QueryStream(query, [`%${req.query.term}%`]));

    stream.pipe(res);
  } catch (err) {
    console.error(err);
    res.status(500).json({ error: 'Internal server error' });
  } finally {
    client.release();
  }
});


const redis = require('redis');
const redisClient = redis.createClient();

app.post('/search', async (req, res) => {
  const { query } = req.body;
  const cacheKey = `search:${query}`;

  redisClient.get(cacheKey, async (err, cachedData) => {
    if (cachedData) {
      return res.json(JSON.parse(cachedData));
    }

    const result = await pool.query(
      `SELECT * FROM intv_XXXXXXXX WHERE data::text ILIKE $1`,
      [`%${query}%`]
    );

    redisClient.setex(cacheKey, 3600, JSON.stringify(result.rows)); // Cache for 1 hour
    res.json(result.rows);
  });
});


SELECT * FROM pg_available_extensions WHERE name = 'pg_trgm';

CREATE EXTENSION IF NOT EXISTS pg_trgm;

CREATE INDEX data_pg_trgm_idx ON intv_XXXXXXXX USING GIN (data gin_trgm_ops);

SELECT * FROM intv_XXXXXXXX WHERE data::text ILIKE '%search_term%';

CREATE INDEX data_jsonb_idx ON intv_XXXXXXXX USING GIN (data jsonb_path_ops);


npm install pg-query-stream


const { Pool } = require('pg');
const QueryStream = require('pg-query-stream');
const { pipeline } = require('stream');

const pool = new Pool({
  user: 'your_username',
  host: 'your_host',
  database: 'your_database',
  password: 'your_password',
  port: 5432, // Default PostgreSQL port
});

async function streamData(req, res) {
  const client = await pool.connect();

  try {
    // Define your query
    const sql = `SELECT * FROM intv_XXXXXXXX WHERE data::text ILIKE $1`;
    const params = [`%${req.query.search_term}%`];

    // Create a QueryStream instance
    const queryStream = new QueryStream(sql, params);

    // Execute the query as a stream
    const stream = client.query(queryStream);

    // Pipe the stream to the response
    pipeline(stream, res, (err) => {
      if (err) {
        console.error('Streaming error:', err);
        res.status(500).send('Streaming error');
      }
    });
  } catch (err) {
    console.error('Database error:', err);
    res.status(500).send('Database error');
  } finally {
    client.release();
  }
}

// Example Express route
const express = require('express');
const app = express();

app.get('/stream', streamData);

app.listen(3000, () => {
  console.log('Server is running on port 3000');
});


node:_http_outgoing:945
    throw new ERR_INVALID_ARG_TYPE(
    ^

TypeError [ERR_INVALID_ARG_TYPE]: The "chunk" argument must be of type string or an instance of Buffer or Uint8Array. 
Received an instance of Object



const { Pool } = require('pg');
const QueryStream = require('pg-query-stream');
const { Transform } = require('stream');
const { pipeline } = require('stream');

const pool = new Pool({
  user: 'your_username',
  host: 'your_host',
  database: 'your_database',
  password: 'your_password',
  port: 5432, // Default PostgreSQL port
});

async function streamData(req, res) {
  const client = await pool.connect();

  try {
    const sql = `SELECT * FROM intv_XXXXXXXX WHERE data::text ILIKE $1`;
    const params = [`%${req.query.search_term}%`];

    // Create a QueryStream instance
    const queryStream = new QueryStream(sql, params);

    // Execute the query as a stream
    const dbStream = client.query(queryStream);

    // Transform the stream into JSON strings
    const jsonTransform = new Transform({
      objectMode: true,
      transform(chunk, encoding, callback) {
        callback(null, JSON.stringify(chunk) + '\n'); // Convert to JSON and add a newline
      },
    });

    // Set response headers for streaming JSON
    res.setHeader('Content-Type', 'application/json');

    // Pipe the database stream -> JSON transform -> HTTP response
    pipeline(dbStream, jsonTransform, res, (err) => {
      if (err) {
        console.error('Streaming error:', err);
        res.status(500).send('Streaming error');
      }
    });
  } catch (err) {
    console.error('Database error:', err);
    res.status(500).send('Database error');
  } finally {
    client.release();
  }
}

// Example Express route
const express = require('express');
const app = express();

app.get('/stream', streamData);

app.listen(3000, () => {
  console.log('Server is running on port 3000');
});




const express = require('express');
const { Pool } = require('pg');
const redis = require('redis');

const app = express();
app.use(express.json());

// PostgreSQL configuration
const pool = new Pool({
  user: 'your_username',
  host: 'your_host',
  database: 'your_database',
  password: 'your_password',
  port: 5432, // Default PostgreSQL port
});

// Redis configuration
const redisClient = redis.createClient();
redisClient.on('error', (err) => console.error('Redis Error:', err));

// API to search with caching and pagination
app.get('/search', async (req, res) => {
  const { term, page = 1, limit = 10 } = req.query;

  // Calculate offset for pagination
  const offset = (page - 1) * limit;

  // Cache key based on search term and pagination
  const cacheKey = `search:${term}:page:${page}:limit:${limit}`;

  try {
    // Check if data is already in cache
    redisClient.get(cacheKey, async (err, cachedData) => {
      if (err) {
        console.error('Redis Get Error:', err);
        res.status(500).send('Internal server error');
        return;
      }

      if (cachedData) {
        // Send cached data
        console.log('Serving from cache');
        res.json(JSON.parse(cachedData));
      } else {
        // Query database
        const result = await pool.query(
          `SELECT * FROM intv_XXXXXXXX WHERE data::text ILIKE $1 LIMIT $2 OFFSET $3`,
          [`%${term}%`, limit, offset]
        );

        // Cache the result for 1 hour (3600 seconds)
        redisClient.setex(cacheKey, 3600, JSON.stringify(result.rows));

        // Send response
        console.log('Serving from database');
        res.json(result.rows);
      }
    });
  } catch (err) {
    console.error('Database Error:', err);
    res.status(500).send('Internal server error');
  }
});

// Start the server
app.listen(3000, () => {
  console.log('Server running on port 3000');
});

GET /search?term=example&page=1&limit=10

return Promise.reject(new errors_1.ClientClosedError()); ClientClosedError: The client is closed


const express = require('express');
const NodeCache = require('node-cache');
const { Pool } = require('pg'); // PostgreSQL pool for database queries

const app = express();
const port = 3000;

// Initialize PostgreSQL connection
const pool = new Pool({
  user: 'your_username',
  host: 'localhost',
  database: 'your_database',
  password: 'your_password',
  port: 5432,
});

// Initialize cache with a default TTL of 1 hour (3600 seconds)
const cache = new NodeCache({ stdTTL: 3600 });

/**
 * Search endpoint with cache
 */
app.get('/search', async (req, res) => {
  const { term = '', page = 1, limit = 10 } = req.query;
  const offset = (page - 1) * limit;
  const cacheKey = `search:${term}:page:${page}:limit:${limit}`; // Unique key for each query

  try {
    // Check if data exists in cache
    const cachedData = cache.get(cacheKey);
    if (cachedData) {
      console.log('Serving from cache');
      return res.json({ source: 'cache', data: cachedData });
    }

    // Query the database if not in cache
    console.log('Fetching from database');
    const query = `
      SELECT * 
      FROM intv_XXXXXXXX 
      WHERE data::text ILIKE $1 
      LIMIT $2 OFFSET $3
    `;
    const values = [`%${term}%`, parseInt(limit), parseInt(offset)];
    const { rows } = await pool.query(query, values);

    // Store results in cache
    cache.set(cacheKey, rows);

    res.json({ source: 'database', data: rows });
  } catch (error) {
    console.error('Error:', error.message);
    res.status(500).json({ error: 'Internal server error' });
  }
});

/**
 * Endpoint to clear the cache (optional for testing)
 */
app.get('/clear-cache', (req, res) => {
  cache.flushAll();
  res.send('Cache cleared');
});

/**
 * Start the server
 */
app.listen(port, () => {
  console.log(`Server is running on http://localhost:${port}`);
});


multi token search

const express = require('express');
const NodeCache = require('node-cache');
const { Pool } = require('pg'); // PostgreSQL pool for database queries

const app = express();
const port = 3000;

// Initialize PostgreSQL connection
const pool = new Pool({
  user: 'your_username',
  host: 'localhost',
  database: 'your_database',
  password: 'your_password',
  port: 5432,
});

// Initialize cache with a default TTL of 1 hour (3600 seconds)
const cache = new NodeCache({ stdTTL: 3600 });

/**
 * Search endpoint with multiple-word search and cache
 */
app.get('/search', async (req, res) => {
  const { term = '', page = 1, limit = 10 } = req.query;
  const offset = (page - 1) * limit;

  // Generate a unique cache key based on the query parameters
  const cacheKey = `search:${term}:page:${page}:limit:${limit}`;

  try {
    // Check if data exists in cache
    const cachedData = cache.get(cacheKey);
    if (cachedData) {
      console.log('Serving from cache');
      return res.json({ source: 'cache', data: cachedData });
    }

    // Split search term into words for multiple-word search
    const words = term.split(/\s+/).filter(Boolean); // Split by spaces and remove empty strings
    const searchQuery = words.map((_, index) => `data::text ILIKE $${index + 1}`).join(' AND ');
    const values = words.map(word => `%${word}%`).concat([parseInt(limit), parseInt(offset)]);

    // Query the database if not in cache
    console.log('Fetching from database');
    const query = `
      SELECT * 
      FROM intv_XXXXXXXX
      WHERE ${searchQuery}
      LIMIT $${words.length + 1} OFFSET $${words.length + 2}
    `;
    const { rows } = await pool.query(query, values);

    // Store results in cache
    cache.set(cacheKey, rows);

    res.json({ source: 'database', data: rows });
  } catch (error) {
    console.error('Error:', error.message);
    res.status(500).json({ error: 'Internal server error' });
  }
});

/**
 * Endpoint to clear the cache (optional for testing)
 */
app.get('/clear-cache', (req, res) => {
  cache.flushAll();
  res.send('Cache cleared');
});

/**
 * Start the server
 */
app.listen(port, () => {
  console.log(`Server is running on http://localhost:${port}`);
});





/**
 * Search endpoint with multiple-word search and cache
 */
app.get('/multi_search', async (req, res) => {
  const { term = '', page = 1, limit = 100 } = req.query;
  const offset = (page - 1) * limit;

  // Generate a unique cache key based on the query parameters
  const cacheKey = `search:${term}:page:${page}:limit:${limit}`;

  try {
    // Check if data exists in cache
    const cachedData = cache.get(cacheKey);
    if (cachedData) {
      console.log('Serving from cache');
      return res.json({ source: 'cache', data: cachedData });
    }

    // Split search term into words for multiple-word search
    const words = term.split(/\s+/).filter(Boolean); // Split by spaces and remove empty strings
    const searchQuery = words.map((_, index) => `data::text ILIKE $${index + 1}`).join(' AND ');
    const values = words.map(word => `%${word}%`).concat([parseInt(limit), parseInt(offset)]);

    // Query the database if not in cache
    console.log('Fetching from database');
    const query = `
      SELECT data
      FROM bpsi_intv_6rvginkmsnhgprpfmkalqoe1
      WHERE ${searchQuery}
      LIMIT $${words.length + 1} OFFSET $${words.length + 2}
    `;

    console.log("values",values);
    const { rows } = await pool.query(query, values);

    // Store results in cache
    cache.set(cacheKey, rows);

    res.json({ source: 'database', count:rows.length, data: rows });
  } catch (error) {
    console.error('Error:', error.message);
    res.status(500).json({ error: 'Internal server error' });
  }
});




app.get('/multi_search', async (req, res) => {
  const { term = '', page = 1, limit = 100 } = req.query;
  const offset = (page - 1) * limit;

  const cacheKey = `search:${term}:page:${page}:limit:${limit}`;

  try {
    const cachedData = cache.get(cacheKey);
    if (cachedData) {
      console.log('Serving from cache');
      return res.json({ source: 'cache', data: cachedData });
    }

    const words = term.split(/\s+/).filter(Boolean).join(' & '); // Full-text search
    const query = `
      SELECT data
      FROM bpsi_intv_6rvginkmsnhgprpfmkalqoe1
      WHERE document_vector @@ to_tsquery($1)
      LIMIT $2 OFFSET $3
    `;
    const values = [words, parseInt(limit), parseInt(offset)];

    console.log('Fetching from database');
    const { rows } = await pool.query(query);

    cache.set(cacheKey, rows, 3600); // Cache results for 1 hour

    res.json({ source: 'database', count: rows.length, data: rows });
  } catch (error) {
    console.error('Error:', error.message);
    res.status(500).json({ error: 'Internal server error' });
  }
});


CREATE EXTENSION IF NOT EXISTS pg_trgm;



CREATE INDEX data_trgm_idx ON bpsi_intv_6rvginkmsnhgprpfmkalqoe1 USING GIN (data gin_trgm_ops);

ALTER TABLE bpsi_intv_6rvginkmsnhgprpfmkalqoe1 ADD COLUMN document_vector tsvector;
UPDATE bpsi_intv_6rvginkmsnhgprpfmkalqoe1 SET document_vector = to_tsvector('english', data);
CREATE INDEX document_vector_idx ON bpsi_intv_6rvginkmsnhgprpfmkalqoe1 USING GIN (document_vector);


step 1 : ALTER TABLE bpsi_intv_6rvginkmsnhgprpfmkalqoe1 ADD COLUMN document_vector tsvector;
step 2 : UPDATE bpsi_intv_6rvginkmsnhgprpfmkalqoe1 SET document_vector = to_tsvector('english', data);
step 3 : CREATE INDEX document_vector_idx ON bpsi_intv_6rvginkmsnhgprpfmkalqoe1 USING GIN (document_vector);


SELECT * 
FROM bpsi_intv_6rvginkmsnhgprpfmkalqoe1
WHERE document_vector @@ to_tsquery('english', 'word');


SELECT * 
FROM bpsi_intv_6rvginkmsnhgprpfmkalqoe1
WHERE document_vector @@ to_tsquery('english', 'word1:*');

SELECT * 
FROM bpsi_intv_6rvginkmsnhgprpfmkalqoe1
WHERE document_vector @@ to_tsquery('english', 'word1 & word2');


const express = require('express');
const { Pool } = require('pg');
const NodeCache = require('node-cache');

// PostgreSQL connection pool
const pool = new Pool({
  user: 'your_user',
  host: 'your_host',
  database: 'your_database',
  password: 'your_password',
  port: 5432,
});

// Initialize NodeCache
const cache = new NodeCache({ stdTTL: 3600 }); // Cache for 1 hour (3600 seconds)

const app = express();
const PORT = 3000;

app.get('/multi_search', async (req, res) => {
  const { term = '', page = 1, limit = 100 } = req.query;
  const offset = (page - 1) * limit;

  // Create a unique cache key based on query parameters
  const cacheKey = `search:${term}:page:${page}:limit:${limit}`;

  try {
    // Check if the result is already cached
    const cachedData = cache.get(cacheKey);
    if (cachedData) {
      console.log('Serving from cache');
      return res.json({ source: 'cache', data: cachedData });
    }

    // Prepare multi-word search query
    const searchTerm = term.split(/\s+/).join(' & '); // Convert spaces to '&' for multi-word search
    const query = `
      SELECT *
      FROM bpsi_intv_6rvginkmsnhgprpfmkalqoe1
      WHERE document_vector @@ to_tsquery('english', $1)
      LIMIT $2 OFFSET $3;
    `;

    // Query the database
    console.log('Fetching from database');
    const { rows } = await pool.query(query, [searchTerm, parseInt(limit), parseInt(offset)]);

    // Store the results in the cache
    cache.set(cacheKey, rows);

    res.json({ source: 'database', count: rows.length, data: rows });
  } catch (error) {
    console.error('Error:', error.message);
    res.status(500).json({ error: 'Internal server error' });
  }
});

app.listen(PORT, () => {
  console.log(`Server running on http://localhost:${PORT}`);
});





Error: bind message supplies 3 parameters, but prepared statement "" requires 1


app.get('/multi_search', async (req, res) => {
  const { term = '', page = 1, limit = 100 } = req.query;
  const offset = (page - 1) * limit;

  try {
    // Split the term into words
    const words = term.split(/\s+/).filter(Boolean);
    const searchQuery = words.map((_, index) => `document_vector @@ to_tsquery('english', $${index + 1})`).join(' AND ');

    // Construct the full query
    const query = `
      SELECT *
      FROM your_table_name
      WHERE ${searchQuery}
      LIMIT $${words.length + 1} OFFSET $${words.length + 2};
    `;

    // Prepare the parameter values
    const values = [...words, parseInt(limit), parseInt(offset)];

    // Execute the query
    const { rows } = await pool.query(query, values);

    res.json({ data: rows });
  } catch (error) {
    console.error('Error:', error.message);
    res.status(500).json({ error: 'Internal server error' });
  }
});



const express = require('express');
const { Pool } = require('pg');
const NodeCache = require('node-cache');

// Initialize app and cache
const app = express();
const cache = new NodeCache({ stdTTL: 3600, checkperiod: 600 }); // Cache TTL of 1 hour

// PostgreSQL connection pool
const pool = new Pool({
  user: 'your_user',
  host: 'your_host',
  database: 'your_database',
  password: 'your_password',
  port: 5432,
});

app.get('/multi_search', async (req, res) => {
  const { term = '', page = 1, limit = 100 } = req.query;
  const offset = (page - 1) * limit;

  // Create a unique cache key based on query parameters
  const cacheKey = `multi_search:${term}:page:${page}:limit:${limit}`;

  try {
    // Check if the result is already in the cache
    const cachedData = cache.get(cacheKey);
    if (cachedData) {
      console.log('Serving from cache');
      return res.json({ source: 'cache', data: cachedData });
    }

    // Split the term into words
    const words = term.split(/\s+/).filter(Boolean); // Split by spaces and remove empty strings

    if (words.length === 0) {
      return res.status(400).json({ error: 'Search term cannot be empty.' });
    }

    // Construct search query dynamically
    const searchQuery = words
      .map((_, index) => `document_vector @@ to_tsquery('english', $${index + 1})`)
      .join(' AND ');

    const query = `
      SELECT *
      FROM your_table_name
      WHERE ${searchQuery}
      LIMIT $${words.length + 1} OFFSET $${words.length + 2};
    `;

    // Prepare parameter values
    const values = [...words, parseInt(limit), parseInt(offset)];

    console.log('Fetching from database');
    const { rows } = await pool.query(query, values);

    // Store the result in cache
    cache.set(cacheKey, rows);

    res.json({ source: 'database', data: rows });
  } catch (error) {
    console.error('Error:', error.message);
    res.status(500).json({ error: 'Internal server error' });
  }
});

// Start the server
const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
});


SELECT indexname, indexdef
FROM pg_indexes
WHERE tablename = 'your_table_name';


DROP INDEX IF EXISTS document_vector_idx;


<ul>
  {data.map((item) => (
    <li key={item.id}>
      <h2>{item.name}</h2>
      <p>Age: {item.age}</p>
    </li>
  ))}
</ul>



import React, { useState, useEffect } from "react";

const DynamicJsonDisplay = () => {
  const [data, setData] = useState([]);

  useEffect(() => {
    // Simulate fetching dynamic JSON data
    const fetchData = async () => {
      const dynamicJsonData = await fetch("https://jsonplaceholder.typicode.com/users").then((response) =>
        response.json()
      );
      setData(dynamicJsonData);
    };

    fetchData();
  }, []);

  const renderObject = (obj) => {
    return (
      <ul>
        {Object.entries(obj).map(([key, value]) => (
          <li key={key}>
            <strong>{key}:</strong>{" "}
            {typeof value === "object" && value !== null ? renderObject(value) : value.toString()}
          </li>
        ))}
      </ul>
    );
  };

  return (
    <div>
      <h1>Dynamic JSON Data</h1>
      {data.map((item, index) => (
        <div key={index}>
          <h2>Item {index + 1}</h2>
          {renderObject(item)}
        </div>
      ))}
    </div>
  );
};

export default DynamicJsonDisplay;



SELECT *
FROM bpsi_intv_6rvginkmsnhgprpfmkalqoe1
WHERE to_tsvector('english', data->>'key1') @@ to_tsquery('141066')  -- Full-text search on extracted text
   OR data->>'key1' % '141066';  -- Trigram-based partial matching on extracted text


